---
title: "Process Integration"
parent: "integration-use-cases"
menu_order: 2
draft: true
---

This document describes some Best Practices around when a Business Process
expands across two Apps.

In most architectures there are business processes that exceed the functionality
of a single microservice App. In that case, there will be some transactional
data used in that process which needs to be transferred between the involved
microservices.

Process integration means integration of transactional data over multiple apps
or microservices. This is probably the most common form of integration and it
has several flavours:

1.  **Business events**, where some work finishes in one App or Thing and the
    next App should be notified to start the next steps of the process

2.  **Workflow integration**, where a user works in one App and then continues
    the same process in another App, in some cases requiring the worked-on data
    to be transferred to the next App

3.  **Process orchestration**, where at the end of a business event, several
    other systems need to be informed and/or updated

4.  **State Engine**, where we gather a large amount of events related to
    different processes, in order to determine that all processes finish
    correctly

5.  **Case Management**, is an implementation of a human workflow in phases
    maintaining a “case”-object with data. The case can run in one App and use
    process orchestration and act as a state engine, or the Case can be
    partially finalized in other Apps, often using sub-cases.

The integration requirements for transactional data differ from the requirements
for master data and reference data. Transactional data changes often and needs
to be available for other systems quickly. In cases where a workflow spans
multiple systems, the data needs to be available almost instantly for the user
to continue working.

Workflow Integration
--------------------

Workflow integration relates to using data in a single workflow that is executed
by one user, where parts of that workflow exists in separate apps. A user needs
to continue a workflow seamlessly, transitioning from one microservice to the
next.

As a practical example, consider an architecture consisting of an ordering app
and a billing app. The ordering app is responsible for managing customer orders.
The billing app is responsible for generating invoices. A user who creates an
order (in the ordering app) needs to approve the invoice (in the support app)
immediately after creating the order. To support this workflow, an integration
is necessary to transfer the user and data between apps.

Using Process Integration
-------------------------

Process integration is applied when

-   A workflow or business process spans multiple apps

-   Transactional data needs to be shared between apps

-   Other relatively fast-changing data needs to be shared between apps

-   Data changes need to be available quickly in other apps

Process integration is often implemented in an event-driven way, or triggered by
user actions in an app.

### How to use it

To make transactional data generated by one app (the “owner”) available in
another (the “client”), there are two main directions:

-   Store a copy of the data in the other app

-   Retrieve data on demand whenever they are needed

Storing a copy is more fault-tolerant in the sense that when the owner app can’t
be reached temporarily, the client app can continue to operate. It also performs
better, since querying the data means querying the local database instead of a
remote endpoint. So, in cases where the data volume allows this, local copies
are preferred. When the volume is too great, on-demand retrieves must be used.

In this best practice, we will consider the situation where local copies can be
used, since retrieving data on demand is much simpler architecturally. However,
for transactional data the data set is usually too big to retrieve the full set
at once when a transfer occurs, so we will have to work with change sets (or
“deltas”).

Building up a local copy of data can be done either by having the client pull
changes or having the owner push. Both options have advantages and
disadvantages:

| **Push**                                                     | **Pull**                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Orders can be sent immediately on change                     | The support app asks for new orders periodically, so the order information may be slightly outdated (eventual consistency) |
| Commonly transferred per individual object                   | Commonly transferred in batches (all objects that changed since last pull) |
| The owner app must know the apps to push to. Each app that needs data must register itself | The owner app does not need to know which apps are using its data. |
| The owner app must retry pushes on failure                   | The client app can retry or re-pull data orders when necessary |
| Recovering from errors in the data transfer is difficult to do in the owner app | Recovery of errors is easy: have the client app re-pull the data |

For microservices, there is a strong recommendation for services to be
autonomous and decoupled from each other. Additionally, it is typically possible
for business processes in the whole architecture to tolerate small delays in
data integration. This leads to a natural choice for pull-based synchronization,
as it is easier to implement and easier to make fault-tolerant.

Recommendations
---------------

### Recommendation for Business Events

In a microservice architecture transaction, the transactional data is best
shared between apps using a pull-based mechanism.

### Recommendation for Workflow Integration 

When a user executes a business process spanning multiple apps, he needs to be
transferred seamlessly between the apps. This is normally accomplished using
deep links into the client app.

In some cases, transactional data needs to be available instantly and the app
cannot afford to wait for an asynchronous pull-process. When opening the deep
link, the client app should synchronously trigger the existing pull process to
retrieve data on demand.

### Do’s

-   Share transactional data using a pull-based mechanism, publishing data from
    the owning app

-   Pull data relatively often, both to make data available for use quickly and
    to prevent the backlog of changed data from building up

-   Provide a global identifier for shared data

-   Handle deleted data as “soft deletes” in the owning app. That way, data will
    not disappear for client and can be recovered if necessary

-   Consider a strategy to handle deleted data in a client (e.g. mark as deleted
    and keep in the database or remove during the pull). The client is free to
    decide how to do this, separately from the owner,

-   Think of correct error handling – should a client retry, show an error, or
    work on older data?

-   Think of the robustness of the transfer mechanism – is it possible to miss
    changes if an app is down or updates are sent out-of-order? How can an app
    recover from this?

-   Create clear (debug) log messages for the data transfer mechanism (e.g. how
    many records were new, changed, removed).

### Don’ts

-   Apply pull-based process integration patterns when the data volume is too
    great to store a local copy. Consider designing a solution specific to your
    case instead.

-   Create APIs for each technical entity separately. Instead, build an API for
    a meaningful business object tree, transferring multiple related objects
    simultaneously.

Technology options with the Mendix Platform
-------------------------------------------

The Mendix platform supports multiple technologies natively for creating
integrations between apps:

-   REST

-   SOAP

-   OData

To transfer a user from one app to the next in a business process, two options
are available:

-   Page URLs

-   Deep link module

### Transferring Transaction Data or Business Event

Creating a pull-based integration mechanism can be done in using several
technology options, each of which has its own advantages and disadvantages for
this specific case.

|              | **Pro’s**                                                    | **Cons**                                             |
| ------------ | ------------------------------------------------------------ | ---------------------------------------------------- |
| **Batch**    | N/A                                                          | *N/A*                                                |
| **File**     | N/A                                                          | N/A                                                  |
| **Database** | N/A                                                          | N/A                                                  |
| **REST**     | Intended for publishing data More efficient message format (JSON) Reusable in custom widgets | Less support for data schema validation              |
| **SOAP**     | Strong schema support                                        | Intended for operations Verbose message format (XML) |
| **OData**    | Using standard HTTP(S) connectivity.                         | Doesn’t support binary interface.                    |
|              | Part of Mendix Core.                                         |                                                      |

Given the pros and cons in the table above, it is recommended to build these
process integrations using a REST endpoint:

-   The use case fits the intent (publishing data).

-   The JSON format is easier to work with and more efficient than XML.

-   The need for a strong schema is less, since this will be an integration
    between two apps in the same architecture scope. The data model can be kept
    consistent in other ways.

A REST endpoint like this can be used for real-time on-demand data retrieval,
making it relatively easy to switch to that mechanism if the data volume grows
over time.

### Recommendation for End-Points

Source systems for process data should publish a REST service endpoint for that
data. This endpoint can be used by consumers to retrieve the data and store it
locally.

### Continuing workflow in another app

Both options here have advantages and disadvantages:

|                    | **Pros**                                                | **Cons**                                 |
| ------------------ | ------------------------------------------------------- | ---------------------------------------- |
| *Page URLs*        | Built into modeler                                      | Only for pages No parameters possible    |
| *Deep link module* | Can start microflows Very flexible with link parameters | App store module (with platform support) |

Page URLs are very easy to use in the Mendix Platform, but currently only
support opening a page and do not support custom parameters. For this case, more
flexibility is needed, both to be able to trigger integration logic when opening
a link, and to be able to link to specific objects using link parameters.

### Recommendation for Workflow integration

Microservices with business processes spanning multiple apps should use the deep
link module to do process integration.